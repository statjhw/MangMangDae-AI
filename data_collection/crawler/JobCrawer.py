import datetime
import json
import os
import time

import requests
from bs4 import BeautifulSoup
from driver import get_chrome_driver
from dynamodb import save_job_to_dynamodb
from logger import get_logger
from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

logger = get_logger(__name__)

# ì „ì—­ ì¹´ìš´í„° ë³€ìˆ˜ë“¤
NOT_ELEMENT_COUNT = 0
TIMEOUT_EXCEPTION_COUNT = 0
ELEMENT_CLICK_INTERCEPT_COUNT = 0
EXCEPTION_COUNT = 0

class Crawler:
    def __init__(
            self,
            data_path=os.path.join(os.getcwd(), "data_collection", "backup"),
            site_name: str = "wanted"
    ):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:136.0) Gecko/20100101 Firefox/136.0",
            "Referer": "https://www.wanted.co.kr/",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ko,en-US;q=0.9,en;q=0.8",
            "Connection": "keep-alive"
        }
        self.driver = get_chrome_driver()
        if not os.path.exists(data_path):
            os.mkdir(data_path)
        self.data_path = data_path
        self.filenames = {
                    "url_list": os.path.join(data_path, f"{site_name}.url_list.json"),
                }
        self.site_name = site_name
        self.endpoint = f"https://www.{site_name}.co.kr"

        # mapping_table.jsonì—ì„œ ì§ì—… ì¹´í…Œê³ ë¦¬ ì •ë³´ ë¡œë”©
            # mapping_table.json íŒŒì¼ì—ì„œ ì§ì—… ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì •ë³´ ë¡œë“œ
        with open("data_collection/crawler/mapping_table.json") as f:
            raw_mapping = json.load(f)
            
            # ì§ì—… IDë¥¼ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
            self.job_category_id2name = {}
            for parent_category, job_map in raw_mapping.items():
                for job_id, name in job_map.items():
                    self.job_category_id2name[int(job_id)] = name
                    
        logger.info(f"ì§ì—… ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(self.job_category_id2name)}ê°œ í•­ëª©")
        


        # ì„¹ì…˜ëª…ì„ í•„ë“œëª…ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        self.map_section_to_field = {
            "í¬ì§€ì…˜ ìƒì„¸": "position_detail",
            "ì£¼ìš”ì—…ë¬´": "main_tasks",
            "ìê²©ìš”ê±´": "qualifications",
            "ìš°ëŒ€ì‚¬í•­": "preferred_qualifications",
            "í˜œíƒ ë° ë³µì§€": "benefits",
            "ì±„ìš© ì „í˜•": "hiring_process",
            "ê¸°ìˆ  ìŠ¤íƒ â€¢ íˆ´": "tech_stack",
        }

    def requests_get(self, url: str) -> requests.Response:
        with requests.Session() as s:
            response = s.get(url, headers=self.headers)
        return response
    
    def run(self):
        """
        ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜
        """
        logger.info("=== í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ===")
        
        try:
            # 1. URL ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
            logger.info("1ë‹¨ê³„: URL ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘")
            job_dict = self.get_url_list()
            logger.info(f"URL ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(job_dict)}ê°œ ì¹´í…Œê³ ë¦¬")
            
            # 2. ì±„ìš©ê³µê³  ì •ë³´ í¬ë¡¤ë§ ë° DB ì €ì¥
            logger.info("2ë‹¨ê³„: ì±„ìš©ê³µê³  í¬ë¡¤ë§ ë° DB ì €ì¥ ì‹œì‘")
            processed_count = self.crawling_job_info(job_dict)
            
            logger.info("=== í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ===")              
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
        finally:
            # ë“œë¼ì´ë²„ ì¢…ë£Œ
            if hasattr(self, 'driver') and self.driver:
                self.driver.quit()
                logger.info("Chrome driver ì¢…ë£Œ ì™„ë£Œ")

    def scroll_down_page(self, driver) -> str:
        page_source = ""
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            if page_source == driver.page_source:
                break
            else:
                page_source = driver.page_source

        return page_source
    
    def get_url_list(self):
        filename = self.filenames["url_list"]
        driver = self.driver

        job_dict = {}
        if os.path.exists(filename):
            with open(filename) as f:
                job_dict = json.load(f)

        with open("data_collection/crawler/mapping_table.json") as f:
            mapping_table = json.load(f)

        for job_parent_category, job_category_id2name in mapping_table.items():
            for job_category in job_category_id2name:
                if job_category in job_dict:
                    continue

                driver.get(
                    f"{self.endpoint}/wdlist/{job_parent_category}/{job_category}"
                )
                logger.info("job_categoryë¡œ ì´ë™")

                logger.info("scroll_down_page í•¨ìˆ˜ í˜¸ì¶œ ì‹œì‘")
                page_source = self.scroll_down_page(driver)

                try:
                    soup = BeautifulSoup(page_source, "html.parser")
                    ul_element = soup.find("ul", {"data-cy": "job-list"})
                    position_list = [
                        a_tag["href"]
                        for a_tag in ul_element.find_all("a")
                        if a_tag.get("href", "").startswith("/wd/")
                    ]
                except Exception:
                    position_list = []

                job_dict[job_category] = {
                    "position_list": position_list,
                }

                with open(
                    os.path.join(self.data_path, f"{self.site_name}.url_list.json"), "w"
                ) as f:
                    logger.info("wanted.url_list.json íŒŒì¼ì— ì €ì¥")
                    json.dump(job_dict, f)

        return job_dict
    
    def crawling_job_info(self, job_dict=None):
        """
        ì±„ìš©ê³µê³  ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ê³  ë°”ë¡œ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
        get_recruit_content_infoì™€ postprocess í•¨ìˆ˜ë¥¼ í•©ì¹œ ë²„ì „
        """
        processed_count = 0

        logger.info("crawling_job_info í•¨ìˆ˜ ì‹¤í–‰ - íŒŒì¼ ì €ì¥ ì—†ì´ ë°”ë¡œ DB ì €ì¥")
        
        if job_dict is None:
            if os.path.exists(self.filenames["url_list"]):
                with open(self.filenames["url_list"]) as f:
                    job_dict = json.load(f)
            else:
                job_dict = {}
        
        driver = self.driver

        for job_category, job_info in job_dict.items():
            logger.info(f"ì²˜ë¦¬ ì¤‘ì¸ ì§ì—… ì¹´í…Œê³ ë¦¬: {job_category}")
            
            for position_url in job_info["position_list"]:
                try:
                    # ì±„ìš©ê³µê³  í˜ì´ì§€ë¡œ ì´ë™
                    driver.get(f"{self.endpoint}{position_url}")
                    time.sleep(1)
                except Exception as e:
                    logger.error(f"âŒ {position_url} â–¶ï¸ í´ë¦­ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
                    continue
                
                try:
                    # ì‚¬ì§„ì˜ HTML êµ¬ì¡°ì— ë§ê²Œ ë” êµ¬ì²´ì ì¸ ì„ íƒì ì‚¬ìš©
                    primary_selector = "//span[@class='Button_Button__label__J05SX' and text()='ìƒì„¸ ì •ë³´ ë” ë³´ê¸°']/ancestor::button"
                    elements = driver.find_elements(By.XPATH, primary_selector)
                    logger.info(f"ìƒì„¸ ì •ë³´ ë²„íŠ¼ ìš”ì†Œ ê°œìˆ˜: {len(elements)}")
                    
                    current_selector = primary_selector
                    
                    if not elements: 
                        logger.info(f"{position_url} -> ê¸°ë³¸ ì„ íƒìë¡œ ë²„íŠ¼ ìš”ì†Œ ì—†ìŒ")
                        # ëŒ€ì•ˆ ì„ íƒìë“¤ ì‹œë„
                        fallback_selectors = [
                            "//button[contains(@class, 'Button_Button__root')]//span[text()='ìƒì„¸ ì •ë³´ ë” ë³´ê¸°']/..",
                            "//span[text()='ìƒì„¸ ì •ë³´ ë” ë³´ê¸°']/ancestor::button",
                            "//button[.//span[text()='ìƒì„¸ ì •ë³´ ë” ë³´ê¸°']]"
                        ]
                        logger.info(f"fallback_selectors: {fallback_selectors}")
                        for selector in fallback_selectors:
                            try:
                                elements = driver.find_elements(By.XPATH, selector)
                                if elements:
                                    logger.info(f"ëŒ€ì•ˆ ì„ íƒìë¡œ ì°¾ìŒ: {selector}, ìš”ì†Œ ê°œìˆ˜: {len(elements)}")
                                    current_selector = selector
                                    break
                            except Exception as e:
                                logger.warning(f"ëŒ€ì•ˆ ì„ íƒì ì‹¤íŒ¨ {selector}: {e}")
                                continue
                    
                    if elements:
                        wait = WebDriverWait(driver, 5)
                        more_button = wait.until(
                            EC.element_to_be_clickable((By.XPATH, current_selector))
                        )
                        more_button.click()
                        time.sleep(1)

                except TimeoutException:
                    logger.warning(f"{position_url} -> ë²„íŠ¼ì´ 5ì´ˆ ë‚´ì— clickable ìƒíƒœê°€ ë˜ì§€ ì•ŠìŒ")
                except ElementClickInterceptedException:
                    logger.warning(f"ğŸš« {position_url} â–¶ï¸ í´ë¦­ ì‹œ ë‹¤ë¥¸ ìš”ì†Œì— ê°€ë ¤ì§")
                except Exception as e:
                    logger.error(f"âŒ {position_url} â–¶ï¸ í´ë¦­ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
                    
                # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì™€ì„œ ë°”ë¡œ íŒŒì‹±
                page_source = driver.page_source
                soup = BeautifulSoup(page_source, "html.parser")

                # íŒŒì‹± ì‹œì‘
                # Job Title
                try:
                    job_title = soup.find("h1", class_="wds-58fmok").text.strip()
                    logger.info(f"job_title: {job_title}")
                except Exception as e:
                    logger.error(f"job_title ì°¾ê¸° ì‹¤íŒ¨ : {e}")
                try:
                    # Company Name and ID
                    company_name_element = soup.find(
                        "strong", class_="CompanyInfo_CompanyInfo__name__sBeI6"
                    )
                    company_name = (
                        company_name_element.text.strip() if company_name_element else None
                    )
                    company_link = soup.find(
                        "a", class_="JobHeader_JobHeader__Tools__Company__Link__NoBQI"
                    )
                    company_id = (
                        company_link["href"].split("/")[-1] if company_link else None
                    )
                    logger.info(f"company_name: {company_name}")
                    logger.info(f"company_id: {company_id}")
                
                except Exception as e:
                    logger.error(f"company_name ì°¾ê¸° ì‹¤íŒ¨ : {e}")

                # Job Description
                try:
                    job_description_article = soup.find(
                        "article", class_="JobDescription_JobDescription__s2Keo"
                    )
                    detailed_content = {}
                    if job_description_article:
                        # Position Detail
                        position_detail_h2 = job_description_article.find(
                            "h2", class_="wds-16rl0sf"
                        )
                        if (
                            position_detail_h2
                            and position_detail_h2.text.strip() == "í¬ì§€ì…˜ ìƒì„¸"
                        ):
                            position_detail_div = position_detail_h2.find_next_sibling(
                                "div",
                                class_="JobDescription_JobDescription__paragraph__wrapper__WPrKC",
                            )
                            if position_detail_div:
                                position_detail_span = position_detail_div.find(
                                    "span", class_="wds-h4ga6o"
                                )
                                if position_detail_span:
                                    position_detail_text = position_detail_span.get_text(
                                        separator="\n"
                                    ).strip()
                                    detailed_content["position_detail"] = position_detail_text

                        # Subsections
                        section_divs = job_description_article.find_all(
                            "div", class_="JobDescription_JobDescription__paragraph__87w8I"
                        )
                        for section_div in section_divs:
                            h3 = section_div.find("h3", class_="wds-17nsd6i")
                            if h3:
                                section_title = h3.text.strip()
                                content_span = section_div.find("span", class_="wds-h4ga6o")
                                if content_span:
                                    content_text = content_span.get_text(
                                        separator="\n"
                                    ).strip()
                                    content_lines = [
                                        line.strip()
                                        for line in content_text.split("\n")
                                        if line.strip()
                                    ]
                                    if content_lines and content_lines[0].startswith("â€¢"):
                                        items = [
                                            line.lstrip("â€¢ ").strip()
                                            for line in content_lines
                                        ]
                                        detailed_content[
                                            self.map_section_to_field.get(
                                                section_title,
                                                section_title.lower().replace(" ", "_"),
                                            )
                                        ] = items
                                    else:
                                        detailed_content[
                                            self.map_section_to_field.get(
                                                section_title,
                                                section_title.lower().replace(" ", "_"),
                                            )
                                        ] = content_text
                except Exception as e:
                    logger.error(f"detailed_content ì°¾ê¸° ì‹¤íŒ¨ : {e}")

                # ë§ˆê°ì¼
                try:
                    deadline = soup.find("span", class_="wds-1u1yyy").get_text()
                except Exception as e:
                    logger.error(f"deadline ì°¾ê¸° ì‹¤íŒ¨ : {e}")
                
                # ìœ„ì¹˜
                try:
                    location = soup.find("span", class_="wds-1td1qmv").get_text()
                except Exception as e:
                    logger.error(f"location ì°¾ê¸° ì‹¤íŒ¨ : {e}")

                # ê²½ë ¥ ì •ë³´ ì¶”ì¶œ (HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
                try:
                    career_info = soup.find("span", class_="JobHeader_JobHeader__Tools__Company__Info__b9P4Y wds-1pe0q6z").get_text()
                    logger.info(f"career_info: {career_info}")
                except Exception as e:
                    logger.error(f"career_info ì°¾ê¸° ì‹¤íŒ¨ : {e}")

                # ê²°ê³¼ êµ¬ì„±
                result = {
                    "url": f"https://www.wanted.co.kr{position_url}",
                    "crawled_at": datetime.datetime.utcnow().isoformat(),
                    "job_category": job_category,
                    "job_name": self.job_category_id2name.get(
                        int(job_category), job_category
                    ),
                    "title": job_title,
                    "company_name": company_name,
                    "company_id": company_id,
                    "dead_line": deadline,
                    "location": location,
                    "career": career_info,  # ê²½ë ¥ ì •ë³´ ì¶”ê°€
                    **detailed_content,
                }

                # DBì— ë°”ë¡œ ì €ì¥
                save_job_to_dynamodb(result)
                processed_count += 1
                
                logger.info(f"âœ… {position_url} ì²˜ë¦¬ ì™„ë£Œ - DB ì €ì¥ ì„±ê³µ (ì´ {processed_count}ê°œ ì²˜ë¦¬)")

        # ìµœì¢… í†µê³„ ì¶œë ¥
        logger.info("=== í¬ë¡¤ë§ ì™„ë£Œ í†µê³„ ===")
        logger.info(f"ì´ ì²˜ë¦¬ëœ ì±„ìš©ê³µê³ : {processed_count}ê°œ")
        return processed_count

if __name__ == "__main__":
    crawler = Crawler()
    crawler.run()