import datetime
import json
import os
import time

import requests
from bs4 import BeautifulSoup
from driver import get_chrome_driver
from dynamodb import save_job_to_dynamodb
from logger import get_logger
from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

logger = get_logger(__name__)

NOT_ELEMENT_COUNT = 0
TIMEOUT_EXCEPTION_COUNT = 0
ELEMENT_CLICK_INTERCEPT_COUNT = 0
EXCEPTION_COUNT = 0

class Crawler:
    def __init__(
        self,
        data_path=os.path.join(os.getcwd(), "job_integration", "backup"),
        site_name="",
    ):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:136.0) Gecko/20100101 Firefox/136.0",
            "Referer": "https://www.wanted.co.kr/",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ko,en-US;q=0.9,en;q=0.8",
            "Connection": "keep-alive"
        }
        self.driver = get_chrome_driver()
        if not os.path.exists(data_path):
            os.mkdir(data_path)
        self.data_path = data_path
        self.site_name = site_name

        self.info_key2name = {
            "Í≤ΩÎ†•": "career",
            "ÌïôÎ†•": "academic_background",
            "ÎßàÍ∞êÏùº": "deadline",
            "Í∑ºÎ¨¥ÏßÄÏó≠": "location",
        }

        self.filenames = {
            "url_list": os.path.join(self.data_path, f"{self.site_name}.url_list.json"),
            "content_info": os.path.join(
                self.data_path, f"{self.site_name}.content_info.json"
            ),
            "result": os.path.join(self.data_path, f"{self.site_name}.result.jsonl"),
        }

    def requests_get(self, url: str) -> requests.Response:
        with requests.Session() as s:
            response = s.get(url, headers=self.headers)
        return response

    def run(self):
        pass

    def scroll_down_page(self, driver) -> str:
        page_source = ""
        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            if page_source == driver.page_source:
                break
            else:
                page_source = driver.page_source

        return page_source


class WanterCrawler(Crawler):
    def __init__(
        self,
        data_path=os.path.join(os.getcwd(), "job_integration", "backup"),
        site_name="wanted",
    ):
        super().__init__(data_path=data_path, site_name=site_name)
        self.endpoint = "https://www.wanted.co.kr"

        with open("job_integration/mapping_table.json") as f :
            raw_mapping = json.load(f)
            self.job_category_id2name = {
                int(job_id): name
                for parent, job_map in raw_mapping.items()
                for job_id, name in job_map.items()
            }

        self.map_section_to_field = {
            "Ìè¨ÏßÄÏÖò ÏÉÅÏÑ∏": "position_detail",
            "Ï£ºÏöîÏóÖÎ¨¥": "main_tasks",
            "ÏûêÍ≤©ÏöîÍ±¥": "qualifications",
            "Ïö∞ÎåÄÏÇ¨Ìï≠": "preferred_qualifications",
            "ÌòúÌÉù Î∞è Î≥µÏßÄ": "benefits",
            "Ï±ÑÏö© Ï†ÑÌòï": "hiring_process",
            "Í∏∞Ïà† Ïä§ÌÉù ‚Ä¢ Ìà¥": "tech_stack",
        }

    def get_url_list(self):
        filename = self.filenames["url_list"]
        driver = self.driver

        job_dict = {}
        if os.path.exists(filename):
            with open(filename) as f:
                job_dict = json.load(f)

        with open("job_integration/mapping_table.json") as f:
            mapping_table = json.load(f)

        for job_parent_category, job_category_id2name in mapping_table.items():
            for job_category in job_category_id2name:
                if job_category in job_dict:
                    continue

                driver.get(
                    f"{self.endpoint}/wdlist/{job_parent_category}/{job_category}"
                )
                logger.info("job_categoryÎ°ú Ïù¥Îèô")

                logger.info("scroll_down_page Ìï®Ïàò Ìò∏Ï∂ú ÏãúÏûë")
                page_source = self.scroll_down_page(driver)

                try:
                    soup = BeautifulSoup(page_source, "html.parser")
                    ul_element = soup.find("ul", {"data-cy": "job-list"})
                    position_list = [
                        a_tag["href"]
                        for a_tag in ul_element.find_all("a")
                        if a_tag.get("href", "").startswith("/wd/")
                    ]
                except Exception:
                    position_list = []

                job_dict[job_category] = {
                    "page_source": page_source,
                    "position_list": position_list,
                }

                with open(
                    os.path.join(self.data_path, f"{self.site_name}.url_list.json"), "w"
                ) as f:
                    logger.info("wanted.url_list.json ÌååÏùºÏóê Ï†ÄÏû•")
                    json.dump(job_dict, f)

        return job_dict

    def get_recruit_content_info(self, job_dict=None):
        global NOT_ELEMENT_COUNT, TIMEOUT_EXCEPTION_COUNT, ELEMENT_CLICK_INTERCEPT_COUNT, EXCEPTION_COUNT
        logger.info("get_recruit_content_info Ìï®Ïàò Ïã§Ìñâ")
        if job_dict is None:
            if os.path.exists(self.filenames["url_list"]):
                with open(self.filenames["url_list"]) as f:
                    job_dict = json.load(f)
            else:
                job_dict = {}

        filename = self.filenames["content_info"]
        driver = self.driver

        position_content_dict = {}
        if os.path.exists(filename):
            with open(filename) as f:
                position_content_dict = json.load(f)

        for job_category, job_info in job_dict.items():
            if job_category in position_content_dict:
                continue

            content_dict = {}
            for position_url in job_info["position_list"]:
                driver.get(f"{self.endpoint}{position_url}")
                time.sleep(1)

                try:  # Ï∂îÍ∞Ä Ï†ïÎ≥¥Î•º ÏúÑÌï¥ ÎçîÎ≥¥Í∏∞ Ï∞Ω ÌÅ¥Î¶≠
                    elements = driver.find_elements(By.XPATH, "//span[text()='ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Îçî Î≥¥Í∏∞']/ancestor::button")
                    logger.info(f"ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Î≤ÑÌäº ÏöîÏÜå Í∞úÏàò: {len(elements)}")

                    if not elements: 
                        logger.info(f"{position_url} -> Î≤ÑÌäº ÏöîÏÜå ÏóÜÏùå (find_elements Í∏∞Ï§Ä)")
                        NOT_ELEMENT_COUNT += 1
                    else:
                        wait = WebDriverWait(driver, 5)
                        more_button = wait.until(
                            EC.element_to_be_clickable(
                                (
                                    By.XPATH,
                                    "//span[text()='ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Îçî Î≥¥Í∏∞']/ancestor::button",
                                )
                            )
                        )
                        more_button.click()
                        logger.info(f"{position_url}Ïùò ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Îçî Î≥¥Í∏∞ Î≤ÑÌäº ÌÅ¥Î¶≠")
                        time.sleep(1)  # ÌÅ¥Î¶≠ ÌõÑ Îç∞Ïù¥ÌÑ∞Í∞Ä Î°úÎìúÎêòÎèÑÎ°ù ÏïΩÍ∞Ñ ÎåÄÍ∏∞

                except TimeoutException:
                    logger.warning(f" {position_url} -> Î≤ÑÌäºÏù¥ 5Ï¥à ÎÇ¥Ïóê clickable ÏÉÅÌÉúÍ∞Ä ÎêòÏßÄ ÏïäÏùå")
                    TIMEOUT_EXCEPTION_COUNT += 1
                except ElementClickInterceptedException:
                    logger.warning(f"üö´ {position_url} ‚ñ∂Ô∏è ÌÅ¥Î¶≠ Ïãú Îã§Î•∏ ÏöîÏÜåÏóê Í∞ÄÎ†§Ïßê (Intercepted)")
                    ELEMENT_CLICK_INTERCEPT_COUNT += 1
                except Exception as e:
                    logger.error(f"‚ùå {position_url} ‚ñ∂Ô∏è ÌÅ¥Î¶≠ Ï§ë Ïïå Ïàò ÏóÜÎäî Ïò§Î•ò Î∞úÏÉù: {e}")
                    EXCEPTION_COUNT += 1

                content_dict[position_url] = driver.page_source
                logger.info(f"{position_url}Ïùò Ï±ÑÏö©Í≥µÍ≥†Î°ú Ïù¥Îèô")
            position_content_dict[job_category] = content_dict

            with open(
                os.path.join(self.data_path, f"{self.site_name}.content_info.json"), "w"
            ) as f:
                json.dump(position_content_dict, f)
                logger.info(f"{job_category}Ïùò content_info.json Ï†ÄÏû•")

        return position_content_dict

    def postprocess(self, position_content_dict=None):
        if position_content_dict is None:
            if os.path.exists(self.filenames["content_info"]):
                with open(self.filenames["content_info"]) as f:
                    position_content_dict = json.load(f)
            else:
                position_content_dict = self.get_recruit_content_info()

        file = open(self.filenames["result"], "w")

        postprocess_dict = {}
        if os.path.exists(self.filenames["content_info"]):
            with open(self.filenames["content_info"]) as f:
                postprocess_dict = json.load(f)

        # Process each job posting
        for job_category, info_dict in position_content_dict.items():
            for url, content in info_dict.items():
                soup = BeautifulSoup(content, "html.parser")

                # Job Title
                job_title = (
                    soup.find("h1", class_="wds-jtr30u").text.strip()
                    if soup.find("h1", class_="wds-jtr30u")
                    else None
                )

                # Company Name and ID
                company_name_element = soup.find(
                    "strong", class_="CompanyInfo_CompanyInfo__name__sBeI6"
                )
                company_name = (
                    company_name_element.text.strip() if company_name_element else None
                )

                company_link = soup.find(
                    "a", class_="JobHeader_JobHeader__Tools__Company__Link__NoBQI"
                )
                company_id = (
                    company_link["href"].split("/")[-1] if company_link else None
                )

                # Tags
                tags_article = soup.find(
                    "article", class_="CompanyTags_CompanyTags__OpNto"
                )
                tag_name_list = []
                tag_id_list = []
                if tags_article:
                    tag_buttons = tags_article.find_all(
                        "button", class_="Button_Button__root__MS62F"
                    )
                    for tag_button in tag_buttons:
                        tag_name_span = tag_button.find("span", class_="wds-1m3gvmz")
                        if tag_name_span:
                            tag_name = tag_name_span.text.strip()
                            tag_id = tag_button.get("data-tag-id")
                            if tag_name and tag_id:
                                tag_name_list.append(tag_name)
                                tag_id_list.append(tag_id)

                # Job Description
                job_description_article = soup.find(
                    "article", class_="JobDescription_JobDescription__s2Keo"
                )
                detailed_content = {}
                if job_description_article:
                    # Position Detail
                    position_detail_h2 = job_description_article.find(
                        "h2", class_="wds-qfl364"
                    )
                    if (
                        position_detail_h2
                        and position_detail_h2.text.strip() == "Ìè¨ÏßÄÏÖò ÏÉÅÏÑ∏"
                    ):
                        position_detail_div = position_detail_h2.find_next_sibling(
                            "div",
                            class_="JobDescription_JobDescription__paragraph__wrapper__WPrKC",
                        )
                        if position_detail_div:
                            position_detail_span = position_detail_div.find(
                                "span", class_="wds-wcfcu3"
                            )
                            if position_detail_span:
                                position_detail_text = position_detail_span.get_text(
                                    separator="\n"
                                ).strip()
                                detailed_content["position_detail"] = (
                                    position_detail_text
                                )

                    # Subsections
                    section_divs = job_description_article.find_all(
                        "div", class_="JobDescription_JobDescription__paragraph__87w8I"
                    )
                    logger.info(f"section_divÏùò Í∞ØÏàò: {len(section_divs)}")
                    for section_div in section_divs:
                        h3 = section_div.find("h3", class_="wds-1y0suvb")
                        if h3:
                            section_title = h3.text.strip()
                            logger.info(section_title)
                            content_span = section_div.find("span", class_="wds-wcfcu3")
                            if content_span:
                                content_text = content_span.get_text(
                                    separator="\n"
                                ).strip()
                                content_lines = [
                                    line.strip()
                                    for line in content_text.split("\n")
                                    if line.strip()
                                ]
                                if content_lines and content_lines[0].startswith("‚Ä¢"):
                                    items = [
                                        line.lstrip("‚Ä¢ ").strip()
                                        for line in content_lines
                                    ]
                                    detailed_content[
                                        self.map_section_to_field.get(
                                            section_title,
                                            section_title.lower().replace(" ", "_"),
                                        )
                                    ] = items
                                else:
                                    detailed_content[
                                        self.map_section_to_field.get(
                                            section_title,
                                            section_title.lower().replace(" ", "_"),
                                        )
                                    ] = content_text

                # ÎßàÍ∞êÏùº
                try:
                    deadline = soup.find("span", class_="wds-lgio6k").get_text()
                except:
                    deadline = "no_data"
                # ÏúÑÏπò
                try:
                    location = soup.find("span", class_="wds-1o4yxuk").get_text()
                except:
                    location = "no_data"

                # Construct result
                result = {
                    "url": f"https://www.wanted.co.kr{url}",
                    "crawled_at": datetime.datetime.utcnow().isoformat(),
                    "job_category": job_category,
                    "job_name": self.job_category_id2name.get(
                        int(job_category), job_category
                    ),
                    "title": job_title,
                    "company_name": company_name,
                    "company_id": company_id,
                    "tag_name": tag_name_list,
                    "tag_id": tag_id_list,
                    "dead_line": deadline,
                    "location": location,
                    **detailed_content,
                }
                save_job_to_dynamodb(result)
                # Write to file
                file.write(json.dumps(result, ensure_ascii=False) + "\n")

        file.close()

    def run(self):
        job_dict = self.get_url_list()
        position_content_dict = self.get_recruit_content_info(job_dict)
        result_dict = self.postprocess(position_content_dict)
        logger.info(f"Î≤ÑÌäº ÏóÜÏùå: {NOT_ELEMENT_COUNT}")
        logger.info(f"Timeout: {TIMEOUT_EXCEPTION_COUNT}")
        logger.info(f"ÌÅ¥Î¶≠ Ï∞®Îã®: {ELEMENT_CLICK_INTERCEPT_COUNT}")
        logger.info(f"Í∏∞ÌÉÄ Ïò§Î•ò: {EXCEPTION_COUNT}")
        return result_dict
