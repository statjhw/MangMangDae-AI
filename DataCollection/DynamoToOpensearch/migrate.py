#!/usr/bin/env python3
"""
DynamoDBì—ì„œ OpenSearchë¡œ ë°ì´í„°ë¥¼ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ì„ë² ë”© í¬í•¨)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” DynamoDBì˜ wanted_jobs í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì™€
ì „ì²˜ë¦¬, ì„ë² ë”© ìƒì„± í›„ OpenSearch ì¸ë±ìŠ¤ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
"""

import sys
import os
import time
import torch
from datetime import datetime
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from DB.dynamodb import DynamoDB
from DB.opensearch import OpenSearchDB

from DB.logger import setup_logger
from langchain_huggingface import HuggingFaceEmbeddings

from data_preprocessing import JobDataPreprocessor

# ë¡œê±° ì„¤ì •
logger = setup_logger(__name__)


class DynamoToOpenSearchMigrator:
    def __init__(self, batch_size: int = 50):  # ì„ë² ë”© ë•Œë¬¸ì— ë°°ì¹˜ í¬ê¸° ì¤„ì„
        """
        ë§ˆì´ê·¸ë ˆì´ì…˜ í´ë˜ìŠ¤ ì´ˆê¸°í™” (ì„ë² ë”© í¬í•¨)
        
        Args:
            batch_size (int): í•œ ë²ˆì— ì²˜ë¦¬í•  ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 50)
        """
        self.dynamodb = DynamoDB()
        self.opensearch = OpenSearchDB()
        self.batch_size = batch_size
        self.migrated_count = 0
        self.error_count = 0
        
        # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.preprocessor = JobDataPreprocessor()
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = self._initialize_embedding_model()
        
        logger.info(f"Migrator initialized with batch size: {batch_size}")
        logger.info(f"Embedding model: intfloat/multilingual-e5-large")
        logger.info(f"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}")
    
    def _initialize_embedding_model(self) -> HuggingFaceEmbeddings:
        """
        HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        
        Returns:
            ì´ˆê¸°í™”ëœ ì„ë² ë”© ëª¨ë¸
        """
        logger.info("Initializing embedding model...")
        try:
            model = HuggingFaceEmbeddings(
                model_name="intfloat/multilingual-e5-large",
                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            logger.info("âœ… Embedding model initialized successfully")
            return model
        except Exception as e:
            logger.error(f"âŒ Error initializing embedding model: {e}")
            logger.info("Retrying model initialization...")
            return HuggingFaceEmbeddings(
                model_name="intfloat/multilingual-e5-large",
                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
    
    def test_connections(self) -> bool:
        """
        DynamoDBì™€ OpenSearch ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ëª¨ë“  ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€
        """
        logger.info("Testing connections...")
        
        # DynamoDB ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            table = self.dynamodb.dynamodb.Table(self.dynamodb.table_name)
            table.load()
            logger.info(f"âœ… DynamoDB connection successful. Table: {self.dynamodb.table_name}")
        except Exception as e:
            logger.error(f"âŒ DynamoDB connection failed: {str(e)}")
            return False
        
        # OpenSearch ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            if self.opensearch.test_connection():
                logger.info("âœ… OpenSearch connection successful")
            else:
                logger.error("âŒ OpenSearch connection failed")
                return False
        except Exception as e:
            logger.error(f"âŒ OpenSearch connection failed: {str(e)}")
            return False
        
        logger.info("âœ… All connections tested successfully")
        return True
    
    def transform_document_with_embedding(self, dynamo_item: Dict[str, Any]) -> Dict[str, Any]:
        """
        DynamoDB ì•„ì´í…œì„ ì „ì²˜ë¦¬í•˜ê³  ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ OpenSearch ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            dynamo_item (dict): DynamoDBì—ì„œ ê°€ì ¸ì˜¨ ì•„ì´í…œ
            
        Returns:
            dict: OpenSearchìš©ìœ¼ë¡œ ë³€í™˜ëœ ë¬¸ì„œ (ì„ë² ë”© í¬í•¨)
        """
        url = dynamo_item.get('url', 'Unknown URL')
        
        try:
            # 1. ê¸°ë³¸ í•„ë“œ ë§¤í•‘
            transformed = {
                'url': dynamo_item.get('url', ''),
                'title': dynamo_item.get('title', ''),
                'company_name': dynamo_item.get('company_name', ''),
                'company_id': dynamo_item.get('company_id', ''),
                'location': dynamo_item.get('location', ''),
                'job_name': dynamo_item.get('job_name', ''),
                'job_category': dynamo_item.get('job_category', ''),
                'dead_line': dynamo_item.get('dead_line', ''),
                'crawled_at': dynamo_item.get('crawled_at', ''),
            }
            
            # íƒœê·¸ ì •ë³´
            
            # ìƒì„¸ ë‚´ìš© í•„ë“œë“¤
            detail_fields = [
                'position_detail', 'main_tasks', 'qualifications',
                'preferred_qualifications', 'benefits', 'hiring_process'
            ]
            
            for field in detail_fields:
                if field in dynamo_item and dynamo_item[field]:
                    transformed[field] = dynamo_item[field]
            
            # ë‚ ì§œ í•„ë“œ ì²˜ë¦¬
            if 'crawled_at' in dynamo_item and dynamo_item['crawled_at']:
                transformed['created_at'] = dynamo_item['crawled_at']
            else:
                transformed['created_at'] = datetime.now().isoformat()
            transformed['updated_at'] = datetime.now().isoformat()
            
            # 2. ì „ì²˜ë¦¬ ìˆ˜í–‰
            logger.info(f"ì „ì²˜ë¦¬ ì‹œì‘: {url}")
            preprocessed_content = self.preprocessor.preprocess(dynamo_item)
            
            if not preprocessed_content:
                logger.warning(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {url}")
                return None
                
            transformed['preprocessed_content'] = preprocessed_content
            logger.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {url} (ê¸¸ì´: {len(preprocessed_content)} ë¬¸ì)")
            
            # 3. ì„ë² ë”© ìƒì„±
            logger.info(f"ì„ë² ë”© ìƒì„± ì‹œì‘: {url}")
            embedding_start_time = time.time()
            
            try:
                # [document] í”„ë¦¬í”½ìŠ¤ ì¶”ê°€ (multilingual-e5-large ëª¨ë¸ìš©)
                embedding_text = f"[document] {preprocessed_content}"
                content_embedding = self.embedding_model.embed_query(embedding_text)
                transformed['content_embedding'] = content_embedding
                
                embedding_time = time.time() - embedding_start_time
                vector_dim = len(content_embedding) if hasattr(content_embedding, '__len__') else 'unknown'
                
                logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {url} (ì°¨ì›: {vector_dim}, ì†Œìš”ì‹œê°„: {embedding_time:.2f}ì´ˆ)")
                
            except Exception as e:
                embedding_time = time.time() - embedding_start_time
                logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {url} - {str(e)} (ì†Œìš”ì‹œê°„: {embedding_time:.2f}ì´ˆ)")
                return None
                
            return transformed
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ë³€í™˜ ì‹¤íŒ¨: {url} - {str(e)}")
            return None
    
    def migrate_batch_with_embedding(self, documents: List[Dict[str, Any]]) -> bool:
        """
        ë¬¸ì„œ ë°°ì¹˜ë¥¼ ì„ë² ë”©ê³¼ í•¨ê»˜ OpenSearchë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
        
        Args:
            documents (list): ë§ˆì´ê·¸ë ˆì´ì…˜í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        batch_start_time = time.time()
        logger.info(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ")
        
        try:
            # ë¬¸ì„œ ë³€í™˜ ë° ì„ë² ë”© ìƒì„±
            transformed_docs = []
            doc_ids = []
            
            for i, doc in enumerate(documents):
                doc_start_time = time.time()
                logger.info(f"ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ({i+1}/{len(documents)}): {doc.get('url', 'Unknown')}")
                
                try:
                    transformed_doc = self.transform_document_with_embedding(doc)
                    if transformed_doc is not None:
                        transformed_docs.append(transformed_doc)
                        # URLì„ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ  ID ìƒì„± (ì¤‘ë³µ ë°©ì§€)
                        doc_id = doc.get('url', f"doc_{self.migrated_count + i + 1}")
                        doc_ids.append(doc_id)
                        
                        doc_time = time.time() - doc_start_time
                        logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ ({i+1}/{len(documents)}): {doc_time:.2f}ì´ˆ")
                    else:
                        doc_time = time.time() - doc_start_time
                        logger.warning(f"âš ï¸ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨, ê±´ë„ˆëœ€ ({i+1}/{len(documents)}): {doc.get('url', 'Unknown')} ({doc_time:.2f}ì´ˆ)")
                        self.error_count += 1
                        
                except Exception as e:
                    doc_time = time.time() - doc_start_time
                    logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({i+1}/{len(documents)}): {str(e)} ({doc_time:.2f}ì´ˆ)")
                    self.error_count += 1
                    continue
            
            if not transformed_docs:
                logger.warning("âš ï¸ ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ì–´ ë°°ì¹˜ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
                return True
            
            # OpenSearchì— ë²Œí¬ ì¸ë±ì‹± (ê³ ìœ  ID í¬í•¨)
            logger.info(f"ğŸ” OpenSearch ì¸ë±ì‹± ì‹œì‘: {len(transformed_docs)}ê°œ ë¬¸ì„œ")
            indexing_start_time = time.time()
            
            response = self.opensearch.bulk_index_with_ids(transformed_docs, doc_ids)
            
            indexing_time = time.time() - indexing_start_time
            batch_total_time = time.time() - batch_start_time
            
            # ì‘ë‹µ í™•ì¸
            if response.get('errors', False):
                errors = response.get('items', [])
                error_count = 0
                for error in errors:
                    if 'index' in error and 'error' in error['index']:
                        logger.error(f"âŒ ë²Œí¬ ì¸ë±ì‹± ì˜¤ë¥˜: {error['index']['error']}")
                        error_count += 1
                self.error_count += error_count
                logger.warning(f"âš ï¸ ë²Œí¬ ì¸ë±ì‹± ì™„ë£Œ ({error_count}ê°œ ì˜¤ë¥˜, ì¸ë±ì‹±: {indexing_time:.2f}ì´ˆ)")
            else:
                logger.info(f"âœ… ë²Œí¬ ì¸ë±ì‹± ì„±ê³µ: {len(transformed_docs)}ê°œ ë¬¸ì„œ (ì¸ë±ì‹±: {indexing_time:.2f}ì´ˆ)")
            
            self.migrated_count += len(transformed_docs)
            logger.info(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: ì´ {batch_total_time:.2f}ì´ˆ (ë¬¸ì„œë‹¹ í‰ê· : {batch_total_time/len(documents):.2f}ì´ˆ)")
            return True
            
        except Exception as e:
            batch_time = time.time() - batch_start_time
            logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)} (ì†Œìš”ì‹œê°„: {batch_time:.2f}ì´ˆ)")
            self.error_count += len(documents)
            return False
    
    def migrate_all_with_embedding(self) -> Dict[str, int]:
        """
        DynamoDBì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬, ì„ë² ë”©ê³¼ í•¨ê»˜ OpenSearchë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
        
        Returns:
            dict: ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ í†µê³„
        """
        logger.info("Starting migration from DynamoDB to OpenSearch with embeddings")
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        if not self.test_connections():
            raise Exception("Connection test failed. Please check your configuration.")
        
        # OpenSearch ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” í™•ì¸ (ì„ë² ë”© ë§¤í•‘ í¬í•¨)
        try:
            self.opensearch.create_index()  # ì´ë¯¸ KNN ë§¤í•‘ì´ í¬í•¨ëœ create_index ì‚¬ìš©
            logger.info("OpenSearch index with embedding mapping created/verified")
            
        except Exception as e:
            logger.error(f"Error creating OpenSearch index: {str(e)}")
            raise
        
        # DynamoDBì—ì„œ ë°ì´í„° ìŠ¤ìº”
        batch = []
        start_time = time.time()
        
        try:
            logger.info(f"Starting data scan from DynamoDB table: {self.dynamodb.table_name}")
            
            for item in self.dynamodb.scan_items_generator(self.dynamodb.table_name, self.batch_size):
                batch.append(item)
                
                # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
                if len(batch) >= self.batch_size:
                    logger.info(f"Processing batch of {len(batch)} documents...")
                    self.migrate_batch_with_embedding(batch)
                    batch = []
                    
                    # ì§„í–‰ ìƒí™© ë¡œê¹…
                    elapsed_time = time.time() - start_time
                    logger.info(f"Progress: {self.migrated_count} documents migrated, "
                              f"{self.error_count} errors, elapsed: {elapsed_time:.2f}s")
            
            # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
            if batch:
                logger.info(f"Processing final batch of {len(batch)} documents...")
                self.migrate_batch_with_embedding(batch)
                
        except Exception as e:
            logger.error(f"Error during migration: {str(e)}")
            raise
        
        # ìµœì¢… í†µê³„
        total_time = time.time() - start_time
        stats = {
            'total_migrated': self.migrated_count,
            'total_errors': self.error_count,
            'total_time_seconds': total_time,
            'migration_rate': self.migrated_count / total_time if total_time > 0 else 0,
            'documents_per_minute': (self.migrated_count / total_time * 60) if total_time > 0 else 0
        }
        
        logger.info(f"Migration completed. Stats: {stats}")
        return stats
    
    def verify_migration_with_search(self, sample_size: int = 5) -> bool:
        """
        ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ë¥¼ ê²€ìƒ‰ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.
        
        Args:
            sample_size (int): ê²€ì¦í•  ìƒ˜í”Œ í¬ê¸°
            
        Returns:
            bool: ê²€ì¦ ì„±ê³µ ì—¬ë¶€
        """
        logger.info(f"Verifying migration with semantic search (sample size: {sample_size})")
        
        try:
            # 1. ì „ì²´ ë¬¸ì„œ ìˆ˜ í™•ì¸
            query = {"query": {"match_all": {}}, "size": 0}
            response = self.opensearch.search(query)
            total_docs = response['hits']['total']['value']
            
            logger.info(f"OpenSearch contains {total_docs} documents")
            
            if total_docs == 0:
                logger.warning("No documents found in OpenSearch")
                return False
            
            # 2. ìƒ˜í”Œ ë¬¸ì„œë“¤ì˜ ì„ë² ë”© ì¡´ì¬ í™•ì¸
            sample_query = {
                "query": {"match_all": {}},
                "size": sample_size,
                "_source": {
                    "includes": ["url", "title", "company_name", "preprocessed_content"],
                    "excludes": ["content_embedding"]  # í° ë²¡í„°ëŠ” ì œì™¸í•˜ê³  í™•ì¸
                }
            }
            
            sample_response = self.opensearch.search(sample_query)
            sample_docs = sample_response['hits']['hits']
            
            # 3. ì„ë² ë”© í•„ë“œ ì¡´ì¬ í™•ì¸ (ì‹¤ì œ ë²¡í„°ëŠ” ì¡°íšŒí•˜ì§€ ì•Šê³  ë§¤í•‘ë§Œ í™•ì¸)
            index_info = self.opensearch.get_index_info()
            mappings = index_info[self.opensearch.index_name]['mappings']
            
            if 'content_embedding' in mappings['properties']:
                logger.info("âœ… Content embedding field exists in index mapping")
                logger.info("âœ… Preprocessed content field confirmed in sample documents")
                
                # ìƒ˜í”Œ ë¬¸ì„œ ì¶œë ¥
                logger.info(f"Sample documents:")
                for i, doc in enumerate(sample_docs[:3], 1):
                    source = doc['_source']
                    logger.info(f"  {i}. {source.get('title', 'N/A')} - {source.get('company_name', 'N/A')}")
                
                return True
            else:
                logger.error("âŒ Content embedding field not found in index mapping")
                return False
                
        except Exception as e:
            logger.error(f"Error verifying migration: {str(e)}")
            return False
    
    def test_semantic_search(self, query_text: str = "í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì React") -> bool:
        """
        ì˜ë¯¸ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        
        Args:
            query_text: í…ŒìŠ¤íŠ¸í•  ê²€ìƒ‰ì–´
            
        Returns:
            bool: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€
        """
        logger.info(f"Testing semantic search with query: '{query_text}'")
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embedding_model.embed_query(f"[Query] {query_text}")
            
            # KNN ê²€ìƒ‰ ì¿¼ë¦¬
            search_body = {
                "query": {
                    "knn": {
                        "content_embedding": {
                            "vector": query_embedding,
                            "k": 5
                        }
                    }
                },
                "size": 5,
                "_source": {
                    "excludes": ["content_embedding"]  # ì‘ë‹µì—ì„œ ë²¡í„° ì œì™¸
                }
            }
            
            response = self.opensearch.search(search_body)
            hits = response['hits']['hits']
            
            if hits:
                logger.info(f"âœ… Semantic search successful! Found {len(hits)} results:")
                for i, hit in enumerate(hits, 1):
                    source = hit['_source']
                    score = hit['_score']
                    logger.info(f"  {i}. [{score:.3f}] {source.get('title', 'N/A')} - {source.get('company_name', 'N/A')}")
                return True
            else:
                logger.warning("âŒ Semantic search returned no results")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Semantic search test failed: {str(e)}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì • í™•ì¸ ë° ì¡°ì •
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',
        force=True  # ê¸°ì¡´ ë¡œê±° ì„¤ì • ë®ì–´ì“°ê¸°
    )
    
    try:
        # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ (ì„ë² ë”© í¬í•¨)
        migrator = DynamoToOpenSearchMigrator(batch_size=25)  # ì„ë² ë”©ìœ¼ë¡œ ì¸í•´ ì‘ì€ ë°°ì¹˜ í¬ê¸°
        
        # ë§ˆì´ê·¸ë ˆì´ì…˜ ìˆ˜í–‰
        stats = migrator.migrate_all_with_embedding()
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ì„ë² ë”© í¬í•¨ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
        print("="*60)
        print(f"ì´ ë§ˆì´ê·¸ë ˆì´ì…˜ëœ ë¬¸ì„œ: {stats['total_migrated']}")
        print(f"ì´ ì˜¤ë¥˜ ìˆ˜: {stats['total_errors']}")
        print(f"ì´ ì†Œìš” ì‹œê°„: {stats['total_time_seconds']:.2f}ì´ˆ")
        print(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì†ë„: {stats['migration_rate']:.2f} ë¬¸ì„œ/ì´ˆ")
        print(f"ë¶„ë‹¹ ì²˜ë¦¬ëŸ‰: {stats['documents_per_minute']:.1f} ë¬¸ì„œ/ë¶„")
        print("="*60)
        
        # ê²€ì¦ ìˆ˜í–‰
        if migrator.verify_migration_with_search():
            print("âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì„±ê³µ")
            
            # ì˜ë¯¸ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            if migrator.test_semantic_search("ë°±ì—”ë“œ ê°œë°œì Python Django"):
                print("âœ… ì˜ë¯¸ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            else:
                print("âš ï¸ ì˜ë¯¸ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        else:
            print("âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì‹¤íŒ¨")
            
    except Exception as e:
        logger.error(f"Migration failed: {str(e)}")
        print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main() 